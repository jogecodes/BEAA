{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pybnesian as pbn\n",
    "import sys\n",
    "import statistics\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import functions\n",
    "functions = reload(functions)\n",
    "\n",
    "(x_train, x_test) = # Load the training and testing datasets as Pandas DataFrames (each row makes for a sample, each column is an attribute)\n",
    "                    # Name of the class variable must be 'class'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of initial training batch\n",
    "original_len = len(x_train)\n",
    "max_len = math.floor(original_len)\n",
    "ini_train_size = max_len\n",
    "\n",
    "# Number of noise nodes \n",
    "noise_dim = 6\n",
    "\n",
    "# Minimum allowed value\n",
    "zero = sys.float_info.min\n",
    "\n",
    "# Mean and variance for noise from which samples will be generated\n",
    "sample_mean = 0\n",
    "sample_variance = 5\n",
    "\n",
    "# Size of samples used in tests (visualization)\n",
    "test_sample_size = 100\n",
    "\n",
    "# Size of each training sample (generation of best samples)\n",
    "generation_size = 100\n",
    "\n",
    "# Max number of training cycles\n",
    "epochs = 20\n",
    "\n",
    "# Logl score parameters: threshold and repulsion/attraction factor\n",
    "logl_threshold = 0\n",
    "logl_rep_attr = 1.1\n",
    "\n",
    "# Size of the training batch for the reverse generator\n",
    "rgen_train_size = 10000\n",
    "\n",
    "# Mean and variance for noise from which rgen will be trained\n",
    "sample_mean_rgen = 0\n",
    "sample_variance_rgen = sample_variance\n",
    "\n",
    "# Number of noise-sampling iterations to calculate mean anomaly score\n",
    "mean_iter = 50\n",
    "\n",
    "# Anomaly score distance power (1=Manhattan, 2=Euclidean...)\n",
    "ano_power = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Net initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1. Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator structure learning and fitting\n",
    "disc_train_data = x_train.sample(ini_train_size)\n",
    "\n",
    "# # Greedy hill-climbing\n",
    "# disc_bn = pbn.hc(disc_train_data, bn_type=pbn.GaussianNetworkType())\n",
    "\n",
    "# PC algorithm\n",
    "learn_algo = pbn.PC()\n",
    "hypothesis = pbn.LinearCorrelation(x_train)\n",
    "disc_graph = learn_algo.estimate(hypot_test = hypothesis, allow_bidirected = False)\n",
    "disc_dag = disc_graph.to_approximate_dag()\n",
    "disc_bn = pbn.GaussianNetwork(disc_dag)\n",
    "disc_bn.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base parameters and structure\n",
    "base_nodes = disc_bn.nodes()\n",
    "base_arcs = disc_bn.arcs()\n",
    "base_cpds = []\n",
    "for node in base_nodes:\n",
    "  base_cpds.append(disc_bn.cpd(node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.1. Discriminator pretest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(x_test['class'])\n",
    "y_score = -1*disc_bn.logl(x_test)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "print('Area under the ROC curve: ' + str(roc_auc))\n",
    "display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='Disc-Logl score')\n",
    "display.plot()\n",
    "# plt.savefig(model_dir+'pretestroc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2. Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise nodes\n",
    "noise_nodes = []\n",
    "for i in range(noise_dim):\n",
    "  noise_nodes.append('noise'+str(i+1))\n",
    "gen_nodes = base_nodes + noise_nodes\n",
    "\n",
    "# Noise arcs\n",
    "noise_arcs = []\n",
    "for nnode in noise_nodes:\n",
    "  for bnode in base_nodes:\n",
    "    noise_arcs.append((nnode,bnode))\n",
    "\n",
    "gen_arcs = noise_arcs\n",
    "gen_bn = functions.reset_gen(noise_nodes, base_nodes, gen_arcs)\n",
    "init_cpds = []\n",
    "\n",
    "# Banned arcs\n",
    "banned_arcs = []\n",
    "for bnode in base_nodes:\n",
    "  for nnode in noise_nodes:\n",
    "    banned_arcs.append((bnode,nnode))\n",
    "\n",
    "banned_edges = []\n",
    "for bnode in base_nodes:\n",
    "  for node in base_nodes:\n",
    "    banned_edges.append((bnode,node))\n",
    "\n",
    "# Adding only random normal noise arcs\n",
    "for node in base_nodes:\n",
    "  noise_parents = noise_nodes\n",
    "  parents = noise_parents\n",
    "  random_betas = []\n",
    "  for i in range(len(noise_parents)):\n",
    "    randnum = np.random.uniform(0,1,1)\n",
    "    if randnum > 0.5:\n",
    "      multiplier = -1\n",
    "    else: \n",
    "      multiplier = 1\n",
    "    random_betas.append(multiplier * np.random.normal(0.5,0.2))\n",
    "  betas = [0] + random_betas\n",
    "  variance = zero\n",
    "  init_cpds.append(pbn.LinearGaussianCPD(node, parents, betas, variance))\n",
    "\n",
    "# Introducing CPDs into generator\n",
    "gen_bn.add_cpds(init_cpds)\n",
    "gen_bn.fitted()\n",
    "\n",
    "# Initial CPDs\n",
    "print('Initial CPDs:')\n",
    "for node in base_nodes:\n",
    "  print(gen_bn.cpd(node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logl_array = []\n",
    "logl_mean_prev = 0\n",
    "\n",
    "pbar = tqdm(total=epochs, position=0, leave=True)\n",
    "\n",
    "for n in range(epochs):\n",
    "  \n",
    "  # Generates batch of samples\n",
    "  batch_samples = functions.gen_samples_genetically(gen_bn, disc_bn, generation_size, sample_mean, sample_variance, generations = 100, p_cross = 0.7, best_frac = 0.5)\n",
    "  batch_samples['sample_logl'] = disc_bn.logl(batch_samples)\n",
    "  logl_mean = batch_samples['sample_logl'].mean(axis=0)\n",
    "  logl_min = batch_samples['sample_logl'].min()\n",
    "  logl_max = batch_samples['sample_logl'].max()\n",
    "\n",
    "  # Gen readjustment\n",
    "  mod_samples = batch_samples.copy()\n",
    "  mod_samples['logl_mod'] = mod_samples.apply(lambda row : functions.logl_mod3(row['sample_logl'], threshold=logl_threshold, rep_attr_factor=logl_rep_attr), axis = 1)\n",
    "  # mod_samples['logl_mod'] = mod_samples.apply(lambda row : functions.logl_mod4(row['sample_logl'], logl_min = logl_min, logl_max = logl_max, rep_attr_factor=logl_rep_attr), axis = 1)\n",
    "  \n",
    "  for nnode in noise_nodes:\n",
    "    mod_samples[nnode] = mod_samples[nnode]*mod_samples['logl_mod']\n",
    "\n",
    "  # gen_bn = functions.reset_gen(noise_nodes, base_nodes, noise_arcs)\n",
    "  # gen_bn.fit(mod_samples)\n",
    "\n",
    "  # gen_bn = functions.learn_condfromnet(learn_data = mod_samples[gen_nodes], interface_nodes = noise_nodes, nodes = base_nodes, forced_arcs = noise_arcs)\n",
    "  gen_bn = functions.learn_condfromnet(learn_data = mod_samples[gen_nodes], interface_nodes = noise_nodes, nodes = base_nodes, banned_arcs = banned_arcs, banned_edges = banned_edges)\n",
    "  assert(gen_bn.fitted())\n",
    "  \n",
    "  logl_array.append(logl_mean)\n",
    "  \n",
    "  pbar.update(1)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1. Gen testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logl evolution\n",
    "plt.plot(logl_array, label=\"Logl\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Log-likelihood')\n",
    "plt.plot([], [], ' ', label='Start logl: ' + str(round(logl_array[0],4)))\n",
    "plt.plot([], [], ' ', label='End logl: ' + str(round(logl_array[-1],4)))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Learnt CPDs\n",
    "print('Learnt CPDs:')\n",
    "for node in base_nodes:\n",
    "  print(gen_bn.cpd(node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reversed noise arcs\n",
    "noise_arcs_reversed = []\n",
    "for nnode in noise_nodes:\n",
    "  for bnode in base_nodes:\n",
    "    noise_arcs_reversed.append((bnode,nnode))\n",
    "\n",
    "# Generating data for rgen training\n",
    "reverse_data = functions.gen_samples(gen_bn, rgen_train_size, sample_mean, sample_variance)\n",
    "\n",
    "# Learning reverse generator\n",
    "# Banned arcs\n",
    "reversed_banned_arcs = []\n",
    "for bnode in base_nodes:\n",
    "  for nnode in noise_nodes:\n",
    "    reversed_banned_arcs.append((nnode,bnode))\n",
    "rgen_bn = functions.learn_condnet(learn_data = reverse_data, interface_nodes = base_nodes, nodes = noise_nodes)\n",
    "assert(rgen_bn.fitted())\n",
    "\n",
    "for node in noise_nodes:\n",
    "  print(rgen_bn.cpd(node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1. By anomaly score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = x_test.copy()\n",
    "\n",
    "for i in range(mean_iter):\n",
    "    noise_sample = rgen_bn.sample(evidence = test_data, concat_evidence = True, ordered = True)\n",
    "    noise_sample = noise_sample.to_pandas()\n",
    "    if i == 0:\n",
    "        ano_score = noise_sample.apply(lambda row : functions.ano_score(row, noise_nodes, power = ano_power), axis = 1)\n",
    "    else:\n",
    "        ano_score = noise_sample.apply(lambda row : functions.ano_score(row, noise_nodes, power = ano_power), axis = 1) + ano_score\n",
    "\n",
    "mean_ano_score = ano_score/mean_iter\n",
    "test_data['ano_score'] = mean_ano_score.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(test_data['class'])\n",
    "y_score_ano = np.array(test_data['ano_score'])\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score_ano)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "print('Area under the ROC curve: ' + str(roc_auc))\n",
    "display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='BayesGEN NoiseAnoScore')\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2. By sample reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = x_test.copy()\n",
    "noise_sample = rgen_bn.sample(evidence = test_data, concat_evidence = True, ordered = True)\n",
    "noise_sample = noise_sample.to_pandas()\n",
    "\n",
    "reconstructed_sample = gen_bn.sample(evidence = noise_sample[noise_nodes], concat_evidence = True, ordered = True).to_pandas()\n",
    "\n",
    "diff_sample = noise_sample.copy()\n",
    "diff_sample[base_nodes] = noise_sample[base_nodes] - reconstructed_sample[base_nodes]\n",
    "modulo = diff_sample.apply(lambda row : functions.euclidean_mod(row, base_nodes), axis = 1)\n",
    "diff_sample['rec_error'] = modulo\n",
    "\n",
    "y_true = np.array(diff_sample['class'])\n",
    "y_score_rec = np.array(diff_sample['rec_error'])\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score_rec)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "print('Area under the ROC curve: ' + str(roc_auc))\n",
    "display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='BayesGEN Rec. Error')\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3. Applying both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_comb = np.multiply(y_score_ano, y_score_rec)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score_comb)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "print('Area under the ROC curve: ' + str(roc_auc))\n",
    "display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='BayesGEN Comb. score')\n",
    "display.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c816c3b6e457c52a01111ad75e1855089bb299a0498be27e079b7d15dad5250"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c428d4300b96006bd89c7861a64ca1379b2ef13eb8daabe066e18431431e18b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
