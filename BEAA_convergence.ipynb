{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pybnesian as pbn\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "import functions\n",
    "import load_functions\n",
    "\n",
    "import functions\n",
    "functions = reload(functions)\n",
    "\n",
    "(x_train, x_test) = # Load the training and testing datasets as Pandas DataFrames (each row makes for a sample, each column is an attribute)\n",
    "                    # Name of the class variable must be 'class'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Características de esta versión:\n",
    "<ul>\n",
    "    <li>Inicializa el generador en forma de toro</li>\n",
    "    <li>Genera las muestras en forma de toro</li>\n",
    "    <li>No fuerza arcos</li>\n",
    "    <li>Mogollón de épocas de entrenamiento</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of noise nodes \n",
    "noise_dim = 15\n",
    "\n",
    "# Mean and variance for initial Gen-BN noise CPDs\n",
    "initial_noise_mean = 0\n",
    "initial_noise_variance = .75\n",
    "\n",
    "# Mean and variance for noise from which samples will be generated\n",
    "sample_mean = .75\n",
    "sample_variance = sample_mean/2\n",
    "\n",
    "# Size of samples used in tests (visualization)\n",
    "test_sample_size = 10\n",
    "\n",
    "# Convergence tolerance for logl difference\n",
    "tol = .01\n",
    "max_iterations = 100000\n",
    "\n",
    "# Size of each training sample (generation of best samples)\n",
    "generation_size = 100000\n",
    "gen_train_size = 10000\n",
    "\n",
    "# Size of the training batch for the reverse generator\n",
    "rgen_train_size = 10000\n",
    "\n",
    "# Mean and variance for noise from which rgen will be trained\n",
    "sample_mean_rgen = 0\n",
    "sample_variance_rgen = sample_variance\n",
    "\n",
    "# Number of noise-sampling iterations to calculate mean anomaly score\n",
    "mean_iter = 10\n",
    "\n",
    "# Anomaly score distance power (1=Manhattan, 2=Euclidean...)\n",
    "ano_power = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model data:')\n",
    "print('Noise nodes: '+str(noise_dim))\n",
    "print('Noise mean and variance: '+'mu='+str(sample_mean)+', sigma='+str(sample_variance))\n",
    "print('Logl convergence tolerance: '+str(tol))\n",
    "print('Max. number of epochs: '+str(max_iterations))\n",
    "print('Generation size: '+str(generation_size))\n",
    "print('Selection size: '+str(gen_train_size)+', ratio: '+str(gen_train_size/generation_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Net initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator (AUC for UNSW goes around 0.97)\n",
    "disc_bn = functions.learn_net(x_train, structure_algorithm='PC')\n",
    "assert(disc_bn.fitted())\n",
    "base_nodes = disc_bn.nodes()\n",
    "\n",
    "y_true = np.array(x_test['class'])\n",
    "y_score = -1*disc_bn.logl(x_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score)\n",
    "disc_roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "print('Disc ROC AUC: '+str(disc_roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_bn.logl(x_test[x_test['class']==0]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_bn.logl(x_test[x_test['class']==1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise nodes\n",
    "noise_nodes = []\n",
    "for i in range(noise_dim):\n",
    "  noise_nodes.append('noise'+str(i+1))\n",
    "\n",
    "# Noise arcs\n",
    "noise_arcs = []\n",
    "for nnode in noise_nodes:\n",
    "  for bnode in base_nodes:\n",
    "    noise_arcs.append((nnode,bnode))\n",
    "\n",
    "# Banned arcs\n",
    "banned_arcs = []\n",
    "for bnode in base_nodes:\n",
    "  for nnode in noise_nodes:\n",
    "    banned_arcs.append((bnode,nnode))\n",
    "\n",
    "# Banned edges\n",
    "banned_edges = []\n",
    "for bnode in base_nodes:\n",
    "  for node in base_nodes:\n",
    "    banned_edges.append((bnode,node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_cpds = functions.create_initial_cpds(base_nodes, noise_nodes, noise_mean = initial_noise_mean, noise_variance = initial_noise_variance, torus = False)\n",
    "gen_bn = functions.reset_interface_net(noise_nodes, base_nodes, noise_arcs)\n",
    "gen_bn.add_cpds(init_cpds)\n",
    "assert(gen_bn.fitted())\n",
    "\n",
    "gen_nodes = noise_nodes + base_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logl_array = []\n",
    "need_training = 1\n",
    "\n",
    "while (need_training and len(logl_array)<max_iterations):\n",
    "  batch_samples = functions.gen_samples(gen_bn, generation_size, sample_mean, sample_variance, torus = True)\n",
    "  batch_samples['sample_logl'] = disc_bn.logl(batch_samples)\n",
    "  logl_mean = batch_samples['sample_logl'].mean(axis=0)\n",
    "  logl_array.append(logl_mean)\n",
    "  print('Iteración: ' +str(len(logl_array)))\n",
    "  print('Logl media: ' +str(logl_mean))\n",
    "  \n",
    "  batch_samples.sort_values('sample_logl', axis=0, ascending = False, inplace = True, ignore_index = True)\n",
    "  selected_samples = batch_samples[0:gen_train_size]\n",
    "  # gen_bn = functions.learn_condnet(learn_data = batch_samples[gen_nodes], interface_nodes = noise_nodes, nodes = base_nodes)\n",
    "  gen_bn = functions.learn_condfromnet(learn_data = selected_samples[gen_nodes], interface_nodes = noise_nodes, nodes = base_nodes, banned_arcs = banned_arcs, banned_edges = banned_edges)\n",
    "  assert(gen_bn.fitted())\n",
    "\n",
    "  if (len(logl_array)>1):\n",
    "    logl_difference = abs(logl_array[-1]-logl_array[-2])\n",
    "    print('Diferencia con la anterior logl: '+ str(logl_difference))\n",
    "    if logl_difference < tol:\n",
    "      need_training = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1. Gen testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logl evolution\n",
    "plt.plot(logl_array, label=\"Logl\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Log-likelihood')\n",
    "plt.plot([], [], ' ', label='Start logl: ' + str(round(logl_array[0],4)))\n",
    "plt.plot([], [], ' ', label='End logl: ' + str(round(logl_array[-1],4)))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Learnt CPDs\n",
    "print('Learnt CPDs:')\n",
    "for node in base_nodes:\n",
    "  print(gen_bn.cpd(node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logl evolution in log scale\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(logl_array, label=\"Logl\")\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Log-likelihood')\n",
    "ax.set_yscale('symlog')\n",
    "ax.set_yticks([-10e25,-10e20,-10e15,-10e10,-10e5,-10])\n",
    "ax.plot([], [], ' ', label='Start logl: ' + str(\"{:.2e}\".format(logl_array[0])))\n",
    "ax.plot([], [], ' ', label='End logl: ' + str(round(logl_array[-1],4)))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reversed noise arcs\n",
    "noise_arcs_reversed = []\n",
    "for nnode in noise_nodes:\n",
    "  for bnode in base_nodes:\n",
    "    noise_arcs_reversed.append((bnode,nnode))\n",
    "\n",
    "# Generating data for rgen training\n",
    "reverse_data = functions.gen_samples(gen_bn, rgen_train_size, sample_mean, sample_variance, torus = True)\n",
    "\n",
    "# Banned arcs\n",
    "reversed_banned_arcs = []\n",
    "for bnode in base_nodes:\n",
    "  for nnode in noise_nodes:\n",
    "    reversed_banned_arcs.append((nnode,bnode))\n",
    "\n",
    "# Learning reverse generator\n",
    "# rgen_bn = functions.learn_condnet(learn_data = reverse_data, interface_nodes = base_nodes, nodes = noise_nodes)\n",
    "# rgen_bn = functions.learn_condfromnet(learn_data = batch_samples[gen_nodes], interface_nodes = base_nodes, nodes = noise_nodes, banned_arcs = reversed_banned_arcs)\n",
    "rgen_bn = functions.learn_condfromnet(learn_data = batch_samples[gen_nodes], interface_nodes = base_nodes, nodes = noise_nodes, banned_arcs = reversed_banned_arcs, banned_edges = banned_edges)\n",
    "assert(rgen_bn.fitted())\n",
    "\n",
    "for node in noise_nodes:\n",
    "  print(rgen_bn.cpd(node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1. By anomaly score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = x_test.copy()\n",
    "\n",
    "for i in range(mean_iter):\n",
    "    noise_sample = rgen_bn.sample(evidence = test_data, concat_evidence = True, ordered = True)\n",
    "    noise_sample = noise_sample.to_pandas()\n",
    "    if i == 0:\n",
    "        ano_score = noise_sample.apply(lambda row : functions.ano_score(row, noise_nodes, power = ano_power), axis = 1)\n",
    "    else:\n",
    "        ano_score = noise_sample.apply(lambda row : functions.ano_score(row, noise_nodes, power = ano_power), axis = 1) + ano_score\n",
    "\n",
    "mean_ano_score = ano_score/mean_iter\n",
    "test_data['ano_score'] = mean_ano_score.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(test_data['class'])\n",
    "y_score_ano = np.array(test_data['ano_score'])\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score_ano)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "print('Area under the ROC curve: ' + str(roc_auc))\n",
    "display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='BayesGEN NoiseAnoScore')\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2. By sample reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = x_test.copy()\n",
    "noise_sample = rgen_bn.sample(evidence = test_data, concat_evidence = True, ordered = True)\n",
    "noise_sample = noise_sample.to_pandas()\n",
    "\n",
    "reconstructed_sample = gen_bn.sample(evidence = noise_sample[noise_nodes], concat_evidence = True, ordered = True).to_pandas()\n",
    "\n",
    "diff_sample = noise_sample.copy()\n",
    "diff_sample[base_nodes] = noise_sample[base_nodes] - reconstructed_sample[base_nodes]\n",
    "modulo = diff_sample.apply(lambda row : functions.euclidean_mod(row, base_nodes), axis = 1)\n",
    "diff_sample['rec_error'] = modulo\n",
    "\n",
    "y_true = np.array(diff_sample['class'])\n",
    "y_score_rec = np.array(diff_sample['rec_error'])\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score_rec)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "print('Area under the ROC curve: ' + str(roc_auc))\n",
    "display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='BayesGEN Rec. Error')\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3. Applying both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_comb = np.multiply(y_score_ano, y_score_rec)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score_comb)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "print('Area under the ROC curve: ' + str(roc_auc))\n",
    "display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='BayesGEN Comb. score')\n",
    "display.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c816c3b6e457c52a01111ad75e1855089bb299a0498be27e079b7d15dad5250"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c428d4300b96006bd89c7861a64ca1379b2ef13eb8daabe066e18431431e18b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
